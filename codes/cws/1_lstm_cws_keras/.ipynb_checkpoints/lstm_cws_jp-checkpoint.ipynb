{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、Bilstm中文分词实验\n",
    "```\n",
    "一、数据集 msr_training.txt（已打好标签），测试集msr_test_gold.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "print(sys.getdefaultencoding())\n",
    "stdi, stdo, stde = sys.stdin, sys.stdout, sys.stderr\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdin, sys.stdout, sys.stderr = stdi, stdo, stde\n",
    "\n",
    "# 第一版中文分词程序：单向lstm\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# 指定显示占用率\n",
    "#import tensorflow as tf\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "#set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "word_size = 128\n",
    "maxlen = 128\n",
    "epochs = 100\n",
    "TRAIN_PERC = 0.9\n",
    "batch_size = 1024 # 1024\n",
    "s = []\n",
    "\n",
    "with open('msr_train_p.txt') as inputs:\n",
    "    line = inputs.readline().strip('\\r\\n')\n",
    "    while(line):\n",
    "        line = line.decode('gbk')\n",
    "        s.append(line)\n",
    "        line = inputs.readline().strip('\\r\\n')\n",
    "t = []\n",
    "with open('msr_test_gold_p.txt') as inputs:\n",
    "    line = inputs.readline().strip('\\r\\n')\n",
    "    while(line):\n",
    "        line = line.decode('gbk')\n",
    "        t.append(line)\n",
    "        line = inputs.readline().strip('\\r\\n')\n",
    "# 下面这个可以滤掉空格行        \n",
    "#f = open('msr_train.txt').read().decode('gbk') # 载入数据        \n",
    "#f = f.split('\\r\\n')\n",
    "#s = []\n",
    "#for line in f:\n",
    "#    if line:\n",
    "#        s.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数： 86918\n",
      "测试集句子数： 3985\n"
     ]
    }
   ],
   "source": [
    "#print(s[20000])\n",
    "print(\"训练集句子数：\",len(s))\n",
    "print(\"测试集句子数：\",len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [] #生成训练样本\n",
    "train_y = []\n",
    "test_x = [] #生成训练样本\n",
    "test_y = []\n",
    "\n",
    "def get_xy(s):  # 把汉字与标签分开\n",
    "    s = re.findall('(.)/(.)', s)\n",
    "    if s:\n",
    "        s = np.array(s)\n",
    "        return list(s[:,0]), list(s[:,1])\n",
    "    \n",
    "def split_data_label(ss):\n",
    "    data = [] \n",
    "    label = []\n",
    "    for i in ss:  \n",
    "        x = get_xy(i)\n",
    "        if x:\n",
    "            data.append(x[0])\n",
    "            label.append(x[1])\n",
    "    return data,label\n",
    "\n",
    "\n",
    "# 训练集汉字与标签分开\n",
    "train_x,train_y = split_data_label(s)\n",
    "\n",
    "# 测试样本\n",
    "test_x,test_y = split_data_label(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扬帆远东做与中国合作的先行[u'\\u626c', u'\\u5e06', u'\\u8fdc', u'\\u4e1c', u'\\u505a', u'\\u4e0e', u'\\u4e2d', u'\\u56fd', u'\\u5408', u'\\u4f5c', u'\\u7684', u'\\u5148', u'\\u884c']\n"
     ]
    }
   ],
   "source": [
    "#a = '人/b  们/e  常/s  说/s  生/b  活/e  是/s  一/s  部/s  教/b  科/m  书/e '\n",
    "#get_xy(a)\n",
    "[print(x,end='') for x in test_x[0]]\n",
    "print(test_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后剩余的训练集句子数 =  85620\n",
      "过滤后剩余的测试集句子数 =  3891\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(index=range(len(train_x)))\n",
    "train['train_x'] = train_x\n",
    "train['train_y'] = train_y\n",
    "train = train[train['train_x'].apply(len) <= maxlen]  # 如果大于maxlen的句子滤掉\n",
    "train.index = range(len(train))\n",
    "print('过滤后剩余的训练集句子数 = ',len(train))\n",
    "\n",
    "test= pd.DataFrame(index=range(len(test_x)))\n",
    "test['test_x'] = test_x\n",
    "test['test_y'] = test_y\n",
    "test = test[test['test_x'].apply(len) <= maxlen]  # 如果大于maxlen的句子滤掉\n",
    "test.index = range(len(test))\n",
    "print('过滤后剩余的测试集句子数 = ',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(train['train_x'])\n",
    "train_y = list(train['train_y'])\n",
    "\n",
    "test_x = list(test['test_x'])\n",
    "test_y = list(test['test_y'])\n",
    "\n",
    "for i,line in enumerate(train_x):\n",
    "    str_tmp = ''\n",
    "    for char in line:\n",
    "        str_tmp+=char+' '\n",
    "    train_x[i] = str_tmp\n",
    "    \n",
    "for i,line in enumerate(train_y):\n",
    "    str_tmp = ''\n",
    "    for char in line:\n",
    "        str_tmp+=char+' '\n",
    "    train_y[i] = str_tmp\n",
    "\n",
    "for i,line in enumerate(test_x):\n",
    "    str_tmp = ''\n",
    "    for char in line:\n",
    "        str_tmp+=char+' '\n",
    "    test_x[i] = str_tmp\n",
    "    \n",
    "for i,line in enumerate(test_y):\n",
    "    str_tmp = ''\n",
    "    for char in line:\n",
    "        str_tmp+=char+' '\n",
    "    test_y[i] = str_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扬 帆 远 东 做 与 中 国 合 作 的 先 行 \n",
      "汉字个数： 5111\n",
      "{'e': 4, 's': 1, 'b': 2, 'm': 3}\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "\n",
    "# token 序列化\n",
    "tokenizer_x = Tokenizer(num_words=None)\n",
    "tokenizer_x.fit_on_texts(np.concatenate((train_x,test_x),axis=0))\n",
    "word_index = tokenizer_x.word_index # 词_索引,字典\n",
    "print(\"汉字个数：\",len(word_index))\n",
    "\n",
    "tokenizer_y = Tokenizer(num_words=None)\n",
    "tokenizer_y.fit_on_texts(['s b m e'])\n",
    "label_index = tokenizer_y.word_index\n",
    "\n",
    "train_x = tokenizer_x.texts_to_sequences(train_x)\n",
    "train_y = tokenizer_y.texts_to_sequences(train_y) \n",
    "\n",
    "test_x = tokenizer_x.texts_to_sequences(test_x)\n",
    "test_y = tokenizer_y.texts_to_sequences(test_y)\n",
    "\n",
    "print(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面的标签序列从1开始，改为从0开始\n",
    "for i,line in enumerate(train_y):\n",
    "    for j,num in enumerate(line):\n",
    "        train_y[i][j] = num-1\n",
    "for i,line in enumerate(test_y):\n",
    "    for j,num in enumerate(line):\n",
    "        test_y[i][j] = num-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 0, 0, 1, 3, 0, 0, 0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 2, 2, 3, 0, 1, 2, 3, 0, 0, 1, 3, 0, 1, 2, 2, 3, 0, 0, 0, 0, 1, 3, 0, 0]\n",
      "[1, 3, 1, 3, 0, 0, 1, 3, 1, 3, 0, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])\n",
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集填充，一个句子的字数小于maxlen，后面填充0\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=maxlen,padding='post')\n",
    "train_y = sequence.pad_sequences(train_y, maxlen=maxlen,padding='post',value=4.)\n",
    "train_y = to_categorical(train_y, num_classes=5)\n",
    "train_y = train_y.reshape(-1,maxlen,5)\n",
    "\n",
    "# 测试集要填充，否则没法用to_categoriecal函数，如果怀疑这样的准确度，可以后面再验证。\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=maxlen,padding='post')\n",
    "test_y = sequence.pad_sequences(test_y, maxlen=maxlen,padding='post',value=4.)\n",
    "test_y = to_categorical(test_y, num_classes=5)\n",
    "test_y = test_y.reshape(-1,maxlen,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85620, 128)\n",
      "(85620, 128, 5)\n",
      "(3891, 128)\n",
      "(85620, 128, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "#print(train_x[0])\n",
    "#print(train_y[0])\n",
    "#print(test_x[0])\n",
    "#print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77058\n",
      "8562\n",
      "77058\n",
      "77058\n",
      "8562\n",
      "8562\n"
     ]
    }
   ],
   "source": [
    "# 打散数据\n",
    "indices = np.arange(train_x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_x = train_x[indices]\n",
    "train_y = train_y[indices]\n",
    "\n",
    "# 取部分数据\n",
    "#train_x=train_x[:10000]\n",
    "#train_y=train_y[:10000]\n",
    "\n",
    "\n",
    "# 分开训练集与验证集\n",
    "len_train = int(train_x.shape[0]*TRAIN_PERC)\n",
    "len_develop = int(train_x.shape[0]-len_train)\n",
    "\n",
    "print(len_train)\n",
    "print(len_develop)\n",
    "\n",
    "develop_x = train_x[len_train:]\n",
    "develop_y = train_y[len_train:]\n",
    "\n",
    "train_x = train_x[:len_train]\n",
    "train_y = train_y[:len_train]\n",
    "\n",
    "print(train_x.shape[0])\n",
    "print(train_y.shape[0])\n",
    "print(develop_x.shape[0])\n",
    "print(develop_y.shape[0])\n",
    "\n",
    "#train_y = train_y.reshape((-1,maxlen,5))\n",
    "#develop_y = develop_y.reshape((-1,maxlen,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设计模型\n",
    "from keras.layers import Dense, Embedding, LSTM, TimeDistributed, Input, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD,RMSprop,Adagrad,Adadelta,Adamax,Adam\n",
    "\n",
    "# 函数式模型\n",
    "sequence = Input(shape=(maxlen,), dtype='int32')\n",
    "embedded = Embedding(len(word_index)+1, word_size, input_length=maxlen, mask_zero=True)(sequence)\n",
    "blstm = Bidirectional(LSTM(64, return_sequences=True), merge_mode='sum')(embedded)\n",
    "output = TimeDistributed(Dense(5, activation='softmax'))(blstm)\n",
    "model = Model(inputs=sequence, outputs=output)\n",
    "#op = Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#op = RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(train_x, train_y, \n",
    "                    validation_data=(develop_x,develop_y),\n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这里的测试集是有填充的，后面可以把填充的去掉试一试\n",
    "score, acc = model.evaluate(test_x, test_y,\n",
    "                            batch_size=batch_size,verbose=2)\n",
    "\n",
    "print('Test score:',score)\n",
    "print('Test accuracy:',acc)\n",
    "\n",
    "# 输出部分结果\n",
    "# 输出结果\n",
    "#predict = model.predict_classes(test_x[:2])\n",
    "#print(predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
