## Attending to Characters in Neural Sequence Labeling Models 

本篇文章是要构造一个与任务无关的序列标注相关的神经网络，并把它结构进字符级的特征信息。尤其是引入了注意力机制。这种方法对没见过的单词判定有积极影响。

<!--more-->

### 2 Bidirectional LSTM for sequence labeling

本论文的模型在Lample et al. (2016) and Rei and Yannakoudakis (2016) 的基础上改进，采用了下面的模型，特殊之处是在模型的后面加上了一层很浅的隐藏层，输出层采用softmax或crf都可以。

![](http://ogtxggxo6.bkt.clouddn.com/iop.png?imageslim)

### 3 Character-level sequence labeling 

![](http://ogtxggxo6.bkt.clouddn.com/nbvd.png?imageslim)



左边是Lample et al.(2016)这篇文章中采用的的结合方法

### 4 Attention over character features

![](http://ogtxggxo6.bkt.clouddn.com/er3.png?imageslim)

看图2右边的图，得到字母向量m之后，不是直接与词向量x进行联合，而是把m与x加权相加，并进行tanh与logitic运算；得到概率z，最后再用互补公式得到标签。

还有一个要改动的地方，就是损失函数要加上一个相似度权重。

![](http://ogtxggxo6.bkt.clouddn.com/56.png?imageslim)

### 5 Dastset

![](http://ogtxggxo6.bkt.clouddn.com/qqw1.png?imageslim)

### result

![](http://ogtxggxo6.bkt.clouddn.com/dd3.png?imageslim)