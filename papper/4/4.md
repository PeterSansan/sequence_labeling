## A Long Dependency Aware Deep Architecture for Joint Chinese Word Segmentation and POS Tagging 

这篇文章是处理中文分词和词性标注联合的方法 ，主要是改善模型复杂度和长期依赖性的问题，模型新颖。

#### 1、通用的方法

![](http://ogtxggxo6.bkt.clouddn.com/opiu.png?imageslim)

可以把整个过程看成是三个阶段：Lookup Table Phase、Encoding Phase、Decoding Phase

编码阶段是求出输入每个字时，每个标签的概率分布情况；译码阶段是求出全局的最优解，用到了维特比译码；

#### 2、长期依赖感知深度架构

![](http://ogtxggxo6.bkt.clouddn.com/qw12.png?imageslim)

##### 2.1 Convolutional Layer

这里的卷积层比较难看懂，不过看懂后确定包含了卷积的含义。转述如下：为了更好地获取复杂的位置信息，使用了卷积层。卷积层的输入是n-gram字向量，如“我是中国人”，当输入汉字为“中”时，即这时的卷积层的输入是“中”，“中国”，“是中国”...，每一个向量维度分别是[d],[2d],[3d]...；通过与一个共同的参数做运行，就可以得到几组维数相同(如100)的输出值，把这几组值串接起来，再通过max pooling，这里是取这组串接值最大的前面100个值为这个汉字的最后的特征向量。当然讲解上从单个汉字的输入输出好理解，但运算是按矩阵来的，所以从数值变换的角度与上面的解释不大一样。如果可以看懂上面的解释再去看论文，会好理解一些。

卷积公式：

![](http://ogtxggxo6.bkt.clouddn.com/erd.png?imageslim)

联接（串接）公式：

![](http://ogtxggxo6.bkt.clouddn.com/zaq.png?imageslim)

池化公式：

![](http://ogtxggxo6.bkt.clouddn.com/etb.png?imageslim)

##### 2.2 Highway Layer

Highway Layer可以在深度网络中保证梯度的值，可以有效的拓展深层网络，加快收敛的速度，和减弱梯度消失的可能。由Srivastava, Greff, and Schmidhuber 2015 提出

![](http://ogtxggxo6.bkt.clouddn.com/ghyt.png?imageslim)

T是转换门，C为带动门。这里采用了一个简单的版本，C=1-T;

![](http://ogtxggxo6.bkt.clouddn.com/ew21.png?imageslim)

##### 2.3 Recurrent Layer

这里的双向LSTM有一个不同的点，就是最后的前向后向的输出不是相加，而是通过联接（串接）（concatention）

![](http://ogtxggxo6.bkt.clouddn.com/yu7.png?imageslim)

#### 3 训练

这里的解码阶段并没有使用维特比译码，而是使用了max-margin criterion(最大间距准则)